(* -*- compile-command: "ocamlbuild -classic-display treemachine.pdf" -*- *)

##verbatim '%' = MathPlugin.mathmode

open Prelude

(*** labels ***)

let s_treemachine = label ~name:\"s:treemachine\" ()
let s_finitetype = label ~name:\"s:finitetype\" ()
let s_lambda = label ~name:\"s:lambda\" ()

(*** doc ***)

let abstract = "A variant of Turing machines is introduced where the tape is replaced by a single tree which can be manipulated in a style akin to purely functional programming. This presents two benefits: first, the extra structure on the tape can be leveraged to write explicit constructions of machines much more easily than with Turing machines. Second, this new kind of machines models finely the asymptotic complexity of functional programming languages, and may allow to answer questions such as ``is this problem inherently slower in functional languages''."

let intro = "{section"Intro"}

This article came to be as I seem to find myself all to often in two kinds of discussions: one of them is the functional programmer's complaint that Turing machine make an unpleasant computation model as it is so unstructured that writing any explicit Turing machine is a chore usually left to the gods of hand-waving. The second one is a common interrogation about some computational problem: ``is it actually slower by a logarithmic factor to solve with a purely functional program, rather than an imperative one''.

My inner functional programmer's reflex would be to turn to {lambda}-calculus to answer such questions. However, there is no denying that it is easier to quantify over automata-like machines, such as Turing-machines, for the purpose of proving complexity results. With that in mind, I will introduce, in this article, a variant of Turing machines, the tree machine, with better structured data which correspond faithfully to the cost-model of purely functional programming languages.

I will not attempt to answer, either positively or negatively, whether concrete problems are slower or not in purely functional style; I will however, give an explicit description of a machine implementing {lambda}-calculus in section~{ref_ s_lambda}, to demonstrate that it is effectively possible to write non-trivial machines explicitly in this model.

{paragraph"Acknowledgment"} I want to thank Alexandre Miquel and Guyslain Naves who, most independently, planted the seed of this article in my mind through very entertaining and enlightening discussions."

let eilenberg = "{section"Eilenberg's machines"}

We will work with a generic notion of machines introduced by Eilenberg~{cite"Eilenberg1974"~extra:"Chapter 10"}, which can be instantiated to yield finite automata as well as Turing machines. The tree machine introduced in Section~{ref_ s_treemachine} is yet another instantiation of Eilenberg's machine (in fact we will give several equivalent definitions).

A type of machine is given by a set <%X%> of {emph"data"} and a set <%Phi SUB sP\(X*X\)%> of {emph"instructions"}. Most of the times the instructions will be partial relations. A machine of type <%(X,Phi)%> is given by a finite set <%Q%> of {emph"states"}, and subsets <%I%> and <%F%> of initial and finite states, as usual for automata. Transitions are labelled with relations of <%Phi%>. A path <%q_0%>,{ldots},<%q_n%> computes the composition of the relations on the successive edges. The machine itself compute the union of the relation computed by path from an initial state to a final state.

It does not change the expressiveness to close <%Phi%> by composition (<%i_1.i_2%>), union (<%i_1+i_2%>), identity (<%r1%>){footnote"In Eilenberg's formulation, a more general kind of relation is considered, in order to be able, typically, to count the multiplicity of successful path. In that case, closure by <%r1%> ({emph"i.e."} adding {epsilon}-transitions) is not permitted."} and empty relation (<%r0%>). We shall use this fact implicitly.

In fact, relations computed by a machine of type <%(X,Phi)%> are exactly the relations in the sub-Kleene algebra of <%sP\(X*X\)%> generated by <%Phi%>: the result for finite automata lifts naturally to Eilenberg's machines. So Eilenberg machines are equivalent to regular expressions with alphabet <%Phi%>. However, if closing <%Phi%> by all the regular expression operations does not change what relation the machines compute, it does change the complexity. Since we are concerned with complexity properties, we may therefore refer to regular expressions as machines, while we will reserve the term {emph"instructions"} to star-free expressions.

This definition of Eilenberg's machines is naturally non-deterministic. It would be more accurate to work with deterministic machines in the setting of this article, but it does not really change anything of substance, and would unnecessarily clutter the presentation. So the machines throughout this article will be non-deterministic, but all of them could be made deterministic, and actually should, for practical applications.
(* arnaud: formellement il faut un moyen de produire l'entr'ee et lire la sortie. *)
(* arnaud: expliquer ce qu'est la composition de relation ? *)
"

let definition = "{section"Tree machines" ~label:s_treemachine}

Let us define the set <%T%> of (rooted, unlabeled, binary) trees as the set generated by the following grammar:
{displaymath begin array [`L;`Sep$~::=~$;`L;`Sep$~{mid}~$;`L] [
  array_line ["<%u%>,<%v%>"; "<%()%>"; "<%(u,v)%>"];
]end}
Such trees will be the data of our tree machines. Take notice of the fact that trees do not replace the alphabet of Turing machines but the whole tape: there will not be a tape of trees, just one tree.

Let us define the following partial functions on <%T%>:
{definition [
  defline "<%delta x%>" "<%(x,x)%>";
  defline "<%pi_1 x%>"  "<%y%>" ~side:"if <%exists z:T, x=(y,z)%>";
  defline "<%pi_2 x%>"  "<%z%>" ~side:"if <%exists y:T, x=(y,z)%>";
  defline "<%(i,j) x%>" "<%(i y,j z)%>" ~side:"if <%x=(y,z)%>, for <%i%> and <%j%> partial functions on <%T%>"; (* arnaud: il faut 'etendre ,ca aux relations en g'en'eral *)
  defline "<%epsilon x%>" "<%()%>";
  defline "<%() x%>" "<%()%>" ~side:"if <%x=()%>";
]}

The set of instruction <%Phi%> is chosen to be the smallest set containing the partial functions <%{delta;pi_1;pi_2;epsilon;();r1}%> and closed by <%(ph,ph)%>. This set of instructions has been chosen to correspond to the presentation of cartesian products and terminal elements in categories as adjunctions.

We call {emph"tree machine"} a machine of type <%(T,Phi)%>. Notice that, contrary to Turing machines, tree machines are not parametrised by an alphabet: the tree structure offers enough power on its own.

Tree machines, by virtue of the <%(ph,ph)%> instruction scheme, has an infinite number of instructions which make it possible to observe the tree and modify it at arbitrary depths. However each individual instruction affects trees at a bounded depth, which is considered a constant time operation in functional language, which is the important property we want to ensure. As we shall see in Section~{ref_ s_finitetype}, this choice of an infinite set of instruction is pure convenience: a finite set suffices.


{subsection"A language of guards and actions"}

As partial functions, the instructions of tree machines can be divided into a guard followed by a reconstruction function. We will adopt a suggestive notation <%gamma => alpha%> where <%gamma%> represents the guard and <%alpha%> the reconstruction function. We may use <%gamma_1 => alpha_1 | gamma_2 => alpha_2%>:
(* arnaud: ne pas oublier de d'efinir le wildcard *)
{definition [
  defline "<%delta%>" "<%x => (x,x)%>";
  defline "<%pi_1%>" "<%(x,\_) => x%>";
  defline "<%pi_2%>" "<%(\_,y) => y%>";
  defline "<%(i,j)%>" "<%(gamma_i,gamma_j) => (alpha_i,alpha_j)%>" ~side:"for <%i=gamma_i=>gamma_j%> and <%j=gamma_j=>alpha_j%>";
  defline "<%epsilon%>" "<%\_ => ()%>";
  defline "<%()%>" "<%() => ()%>";
]}

We require that the guards are linear (we can give a meaning to non-linear patterns but they would not have constant time). First we define pure guards <%gamma => emptyact%>, which only check that <%gamma%> matches the shape of the tree:
(* arnaud: supprimer emptyact *)
{definition [
  defline "<%(x => emptyact)%>" "<%r1%>";
  defline "<%(() => emptyact)%>" "<%()%>";
  defline "<%((gamma_1,gamma_2) => emptyact)%>" "<%((gamma_1 => emptyact),(gamma_2=>emptyact))%>";
]}
We now define generalised projections: instructions of the form <%gamma => x%> where <%x%> is a variable occurring in <%gamma%>:
{definition [
  defline "<%(x => x)%>" "<%r1%>";
  defline "<%((gamma_1,gamma_2) => x)%>" "<%((gamma_1 => x),(gamma_2=>emptyact)).pi_1%>" ~side:"when <%x%> occurs in <%gamma_1%>";
  defline "<%((gamma_1,gamma_2) => x)%>" "<%((gamma_1 => emptyact),(gamma_2=>x)).pi_2%>" ~side:"when <%x%> occurs in <%gamma_2%>";
]}
Finally, we can define generalised guard-and-action instructions <%gamma=>alpha%> (where the variables of <%alpha%> occur in <%gamma%>):
{definition [
  defline "<%(gamma => ())%>" "<%(gamma => emptyact).epsilon%>";
  defline "<%(gamma => (alpha_1,alpha_2))%>" "<%delta.((gamma=>alpha_1),(gamma=>alpha_2))%>";
]}

For example:
{itemize [
  "<%sigma = ((x,y) => (y,x)) = delta.((r1,r1).pi_2,(r1,r1).pi_1)%>";
  "<%push = (((x,y),z) => (x,(y,z))) = delta.(((r1,r1).pi_1,r1).pi_1 , delta.(((r1,r1).pi_2,1).pi_1,((r1,r1),r1).pi_2))%>";
]}
We can apply the small optimisation that <%(1,1).pi_i = pi_i%> -- {emph"i.e."} we make sure that  <%(x,y) => x%> (resp. <%(x,y)=>y%>) is compiled to <%pi_1%> (resp. <%pi_2%>). The examples now read:
{itemize [
  "<%sigma = ((x,y) => (y,x)) = delta.(pi_2,pi_1)%>";
  "<%push = (((x,y),z) => (x,(y,z))) = delta.((pi_1,r1).pi_1 , delta.((pi_2,r1).pi_1,((r1,r1),r1).pi_2))%>";
]}
Further optimisation could consist in testing the shape of the pattern only once and program fast accessors as generalised projections which do not check side conditions. Combined with fact that <%(i,r1).pi_1 = i.pi_1%> (resp. <%(r1,j).pi_2 = j.pi_2%>) we can obtain a fairly compact form for <%push%>:
{itemize[
  "<%push = (((x,y),z) => (x,(y,z))) = ((r1,r1),r1).delta.(pi_1.pi_1 , delta.(pi_2.pi_1,pi_2))%>";
]}

{subsection"Constants"}


It will be useful to embed natural numbers in trees. Any embedding will do. We choose a binary encoding for the sake of compactness:
{definition [
  defline "<%0%>" "<%()%>";
  defline "<%2*n+1%>" "<%((),n)%>";
  defline "<%2*n+2%>" "<%(((),()),n)%>"
 ]}


{subsection"Zipper"}

See~{cite"Huet1997"}. Instructions to traverse a tree by focusing on arbitrary deep sub-trees.

{definition [
  defline "<%open%>" "<%x => ((),x)%>";
  defline "<%left%>" "<%(x,(y,z)) => ((0,(x,z)),y)%>";
  defline "<%right%>" "<%(x,(y,z)) => ((1,(x,y)),z)%>";
  defline "<%up%>" "<%((0,(x,z)),y) => (x,(y,z)) | ((1,(x,y)),z) => (x,(y,z))%>";
  defline "<%exit%>" "<%((),x) => x%>";
]}

The instruction <%zip = up^*.exit%> turns a focused tree back into a tree. Because it uses an iteration, it is not an instruction which has constant time. However, when it is known that the depth of the focused subtree is at most a constant <%n%> we can equivalently use the constant time instruction <%(r1+up^1+%{ldots}%+up^n).exit%>, we may still use <%zip%> as a shorthand.
"

let translations = "{section"Translations"}

{subsection"Turing machines as tree machines"}

It is straightforward to implement Turing machines as tree-machines: fixing a coding for the alphabet, we arrange the tree to be a pair of lists of symbols representing the tape left and right of the head respectively.

All Turing machine instructions are implemented in constant time:
{itemize [
  "Write symbol {mathsf$a$} under the head: <%(x,(\_,y)) => (x,(%{mathsf$a$}%,y)) | (x,()) => (x,(%{mathsf$a$}%,()))%> (the second case extends the tape if we reached the end)";
  "Move right: <%(x,(y,z)) => ((x,y),z)%>";
  "Move left: <%((x,y),z) => (x,(y,z))%>";
  "Check that symbol {mathsf$a$} is under the head: <%(\_,(%{mathsf$a$}%,\_)) => emptyact%>";
]}

So a Turing machine is translated to a tree machine with the same state and transitions labelled as above.

{subsection"A finite type for tree machines"~label:s_finitetype}

The zipper allow us to build a finite set <%Psi%> of instruction which is complete with respect to tree machines, we write <%[[i]]%> for the interpretation of <%i%> in this new set:

{Prelude.definition [
  defline "<%[[()]]%>" "<%(\_,()) => emptyact %>";
  defline "<%[[epsilon]]%>" "<%(x,\_) => (x,())%>";
  defline "<%[[pi_1]]%>" "<%(x,(y,z)) => (x,y)%>";
  defline "<%[[pi_2]]%>" "<%(x,(y,z)) => (x,z)%>";
  defline "<%[[delta]]%>" "<%(x,y) => (x,(y,y))%>";
  defline "<%[[(i,j)]]%>" "<%left.[[i]].up.right.[[j]].up%>";
 ]}

Every instruction <%i IN Phi%> can be implemented as <%open.[[i]].exit%>. The instructions in <%Psi%> observe the tree at depth at most <%3%> (<%up%> gives the upper bound).

(* arnaud:
Remark: more generally, we can implement <%(r,s)%> for any two machines <%r%> and <%s%> as <%left.(r1,r).up.right.(r1,s).up%>.
*)

{subsection"Tree machines as Turing machines"}

Translating tree machines into Turing machines is not as direct as the converse. One way to translate trees into word so that it fits a Turing machine tape is to use the Polish notations: we take the alphabet to be <%{%{mathsf$p$}%;%{mathsf$u$}%}%> (for {emph"pair"} and {emph"unit"} respectively). The tree <%((),((),()))%> is then translated to <%pspss%>.

To manage the Polish notation, we can take a two-tape Turing machine, the first tape holds the current tree, and the second tape can hold a stack to find the boundaries of the two subtrees to implement <%pi_1%> and <%pi_2%> or a buffer with the tree to copy to implement <%delta%>.

The instructions of tree machines are not translated as constant time machines. However, they are all in a polynomial <%P%> of the current size of the tape. Hence, if the complexity of a tree machine is <%O \(f n\)%>, then the corresponding Turing machine has complexity <%O \(P \(f n\) * f n\)%>, which is in the same complexity class.

Therefore, tree machines and Turing machines have the same complexity classes (we only discuss non-deterministic machines in this article, but there is a natural notion of deterministic tree machine, and the translation can be arranged to preserve determinism). However, the translation from tree machines to Turing machines is non-trivial both in term of slowdown of the translated machine and complexity of the translation itself. It would be quite hard to get an explicit description of the translation. On the other hand the translation of Turing machines into tree machines is quite direct. Tree machines are a significant improvement over Turing machines.

{subsection"Tree machines as random access machines"}

A more natural encoding of Tree machines is in random access machines -- another example of Eilenberg machine. In this translation, a tree is an address at this address there is <%0%> if the tree is empty, and <%1%> if the tree is a pair. In the latter the two following addresses contain the addresses of the two subtrees.

In this translation, it is easy to implement the instructions of the tree machine as they are just pointer dereferencing and copy. The difficulties are:
{itemize[
  "Memory allocation: finding empty space to store a tree";
  "Garbage collection: to keep the same memory profile";
]}
Both are linear in a random access machine.

Memory allocation and garbage collection are usually assumed away in modern programming language, making the Tree machine a good complexity model of modern programming languages. On the other hand, it does not have constant-time-access arrays, which the random access machines provides and are available in programming language -- except purely functional ones.

{subsection"Encoding {lambda}-calculus" ~label:s_lambda}

Using explicit substitution {lambda}{upsilon}-calculus~{cite"Lescanne1994"}:
{Prelude.definition[
  defline $({lambda}u)v$ $u[v]$;
  defline $({lambda}u)[s]$ ${lambda}(u[{uparrow_}s])$;
  defline $(u v)[s]$ $(u[s])(v[s])$;
  defline $0[v]$ $v$;
  defline $(n+1)[v]$ $n+1$;
  defline $0[{uparrow_}s]$ $0$;
  defline $(n+1)[{uparrow_}s]$ $n[s][{uparrow}]$;
  defline $n[{uparrow}s]$ $n+1$;
]}

We will use the non-determinism of tree machines to represent the non-determinism of {beta}-reduction (the converse is not possible).

We fix an encoding for the following list of symbols <%{lam;app;succ;nought;subst;term;lift;shift}%>. First we need zipper-like constructions to navigate through terms:
{Prelude.definition [
  defline "<%open%>" "<%u => ((),u)%>";
  defline "<%down_lam%>" "<%(c,(lam,u)) => ((c,lam),u)%>";
  defline "<%down_sigma%>" "<%(c,(subst,(u,s))) => ((c,(subst,s)),u)%>";
  defline "<%left%>" "<%(c,(app,(u,v))) => ((c,(app,(0,v))),u)%>";
  defline "<%right%>" "<%(c,(app,(u,v))) => ((c,(app,(1,u))),v)%>";
  defline "<%up_lam%>" "<%((c,lam),u) => (c,(lam,u))%>";
  defline "<%up_sigma%>" "<%((c,(subst,s)),u) => (c,(subst,(u,s)))%>";
  defline "<%up_app%>" "<%((c,(app,(0,v))),u) => (c,(app,(u,v))) | ((c,(app,(1,u))),v) => (c,(app,(u,v)))%>";
  defline "<%up%>" "<%up_lam+up_sigma+up_app%>";
  defline "<%exit%>" "<%((),u) => u%>";
  defline "<%move%>" "<%down_lam+down_sigma+left+right+up%>";
  defline "<%zip%>" "<%up^*.exit%>";
 ]}
Now we implement the rules in a context independent manner:
{Prelude.definition [
  defline "<%beta%>" "<%(app,((lam,u),v)) => (subst,(u,(term,v)))%>";
  defline "<%lam_sigma%>" "<%(subst,((lam,u),s)) => (lam,(subst,(u,(lift,s))))%>";
  defline "<%app_sigma%>" "<%(subst,((app,(u,v)),s)) => (app,((subst,(u,s)),(subst,(v,s))))%>";
  defline "<%nought_term%>" "<%(subst,(nought,(term,v))) => v%>";
  defline "<%succ_term%>" "<%(subst,((succ,n),(term,\_))) => succ n%>";
  defline "<%nought_lift%>" "<%(subst,(nought,(lift,\_))) => nought%>";
  defline "<%succ_lift%>" "<%(subst,((succ,n),(lift,s))) => (subst,((subst,(x,s)),shift))%>";
  defline "<%var_shift%>" "<%(subst,(nought,shift)) => (succ,nought) | (subst,((succ,n),shift)) => (succ,(succ,n))%>";
  defline "<%rule_sigma%>" "<%lam_sigma+app_sigma+nought_term+succ_term+nought_lift+succ_lift+var_shift%>"; 
  defline "<%rule%>" "<%beta+rule_sigma%>";
]}
A step of {lambda}{upsilon}-calculus is implemented as:
{Prelude.definition[
  defline "<%step%>" "<%open.move^*.(1,rule).zip%>";
]}
Reduction is then <%step^*%>.

To better represent actual {lambda}-calculus, a step of reduction should correspond to exactly one {beta}-reduction and substitutions should be eliminated. To do so we extend or alphabet of symbol with a symbol <%ok%> together with the following rules:
{Prelude.definition[
  defline "<%nought_ok%>" "<%nought => (ok,nought)%>";
  defline "<%succ_ok%>" "<%(succ,n) => (ok,(succ,n))%>";
  defline "<%lam_ok%>" "<%(lam,(ok,u)) => (ok,(lam,u))%>";
  defline "<%app_ok%>" "<%(app,((ok,u),(ok,v))) => (ok,(app,(u,v)))%>";
  defline "<%rule_ok%>" "<%nought_ok+succ_ok+lam_ok+app_ok%>";
  defline "<%check_ok%>" "<%(ok,u) => u%>";
]}
Then, we can encode a step of {lambda}-calculus with:
{Prelude.definition[
  defline "<%step%>" "<%open.move^*.(1,beta.rule_sigma^*.rule_ok^*.check_ok).zip%>";
]}
"

(*** Emit document ***)

open Llncs

let mines = new_institution "MINES ParisTech"

let funding =
  "This research has received funding from the European Research Council under the FP7 grant agreement 278673, Project MemCAD"

let title = {
  title = "The tree machine{command\"thanks\" [A,funding] A}";
  running_title = None
}

let authors = [
  { name = "Arnaud Spiwack";
    email = "arnaud@spiwack.net";
    institution = mines;
    running_name = None;
  };
]

let d = concat [
  intro;
  eilenberg;
  definition;
  translations;
  command \"bibliography\" [A,"library"] A;
]

let packages = [
  "inputenc" , "utf8" ;
  "fontenc" , "T1" ;
  "textcomp", "";
  "microtype" , "" ;
]

let prelude = concat_with_sep [
  usepackage "hyperref";
  text\"\\\\let\\\\oldampersand\\\\&\";
  text\"\\\\renewcommand*\\\\&{{\\\\itshape\\\\oldampersand}}\";
  command \"bibliographystyle\" [T,"plain"] T;
] par

let file = \"treemachine.tex\"


let _ = emit ~file (document
                             ~title
                             ~authors
                             ~abstract
                             ~prelude
                             ~packages
                             d)
