(* -*- compile-command: "ocamlbuild -classic-display treemachine.pdf" -*- *)

##verbatim '%' = MathPlugin.mathmode

open Prelude

(*** labels ***)

let s_treemachine = label ~name:\"s:treemachine\" ()
let s_zipper = label ~name:\"s:zipper\" ()
let s_finitetype = label ~name:\"s:finitetype\" ()
let s_ram = label ~name:\"s:ram\" ()
let s_lambda = label ~name:\"s:lambda\" ()

(*** doc ***)

let abstract = "A variant of Turing machines is introduced where the tape is replaced by a single tree which can be manipulated in a style akin to purely functional programming. This presents two benefits: first, the extra structure on the tape can be leveraged to write explicit constructions of machines much more easily than with Turing machines. Second, this new kind of machines models finely the asymptotic complexity of functional programming languages, and may allow to answer questions such as ``is this problem inherently slower in functional languages''."

let intro = "{section"Intro"}

This article came to be as I seem to find myself all to often in two kinds of discussions: one of them is the functional programmer's complaint that Turing machine make an unpleasant computation model as it is so unstructured that writing any explicit Turing machine is a chore usually left to the gods of hand-waving. The second one is a common interrogation about some computational problem: ``is it actually slower by a logarithmic factor to solve with a purely functional program, rather than an imperative one''.

My inner functional programmer's reflex would be to turn to {lambda}-calculus to answer such questions. However, there is no denying that it is easier to quantify over automata-like machines, such as Turing-machines, for the purpose of proving complexity results. With that in mind, I will introduce, in this article, a variant of Turing machines, the tree machine, with better structured data which correspond faithfully to the cost-model of purely functional programming languages.

I will not attempt to answer, either positively or negatively, whether concrete problems are slower or not in purely functional style; I will however, give an explicit description of a machine implementing {lambda}-calculus in section~{ref_ s_lambda}, to demonstrate that it is effectively possible to write non-trivial machines explicitly in this model.

{paragraph"Acknowledgment"} I want to thank Alexandre Miquel and Guyslain Naves who, most independently, planted the seed of this article in my mind through very entertaining and enlightening discussions."

let eilenberg = "{section"Eilenberg's machines"}

We will work with a generic notion of machines introduced by Eilenberg~{cite"Eilenberg1974"~extra:"Chapter 10"}, which can be instantiated to yield finite automata as well as Turing machines. The tree machine introduced in Section~{ref_ s_treemachine} is yet another instantiation of Eilenberg's machine (in fact we will give several equivalent definitions).

A type of machine is given by a set <%X%> of {emph"data"} and a set <%Phi SUB sP\(X*X\)%> of {emph"instructions"}. Most of the times the instructions will be partial relations. A machine of type <%(X,Phi)%> is given by a finite set <%Q%> of {emph"states"}, and subsets <%I%> and <%F%> of initial and finite states, as usual for automata. Transitions are labelled with relations of <%Phi%>. A path <%q_0%>,{ldots},<%q_n%> computes the composition of the relations on the successive edges. The machine itself compute the union of the relation computed by path from an initial state to a final state.

It does not change the expressiveness to close <%Phi%> by composition (<%i_1.i_2%>), union (<%i_1+i_2%>), identity (<%r1%>){footnote"In Eilenberg's formulation, a more general kind of relation is considered, in order to be able, typically, to count the multiplicity of successful path. In that case, closure by <%r1%> ({emph"i.e."} adding {epsilon}-transitions) is not permitted."} and empty relation (<%r0%>). We shall use this fact implicitly.

In fact, relations computed by a machine of type <%(X,Phi)%> are exactly the relations in the sub-Kleene algebra of <%sP\(X*X\)%> generated by <%Phi%>: the result for finite automata lifts naturally to Eilenberg's machines. So Eilenberg machines are equivalent to regular expressions with alphabet <%Phi%>. However, if closing <%Phi%> by all the regular expression operations does not change what relation the machines compute, it does change the complexity. Since we are concerned with complexity properties, we may therefore refer to regular expressions as machines, while we will reserve the term {emph"instructions"} to star-free expressions.

This definition of Eilenberg's machines is naturally non-deterministic. It would be more accurate to work with deterministic machines in the setting of this article, but it does not really change anything of substance, and would unnecessarily clutter the presentation. So the machines throughout this article will be non-deterministic, but all of them could be made deterministic, and actually should, for practical applications.
(* arnaud: formellement il faut un moyen de produire l'entr'ee et lire la sortie. *)
(* arnaud: expliquer ce qu'est la composition de relation ? *)
"

let definition = "{section"Tree machines" ~label:s_treemachine}

Let us define the set <%T%> of (rooted, unlabeled, binary) trees as the set generated by the following grammar:
{displaymath begin array [`L;`Sep$~::=~$;`L;`Sep$~{mid}~$;`L] [
  array_line ["<%u%>,<%v%>"; "<%()%>"; "<%(u,v)%>"];
]end}
Such trees will be the data of our tree machines. Take notice of the fact that trees do not replace the alphabet of Turing machines but the whole tape: there will not be a tape of trees, just one tree.

Let us define the following partial functions on <%T%>:
{definition [
  defline "<%delta x%>" "<%(x,x)%>";
  defline "<%pi_1 x%>"  "<%y%>" ~side:"if <%exists z:T, x=(y,z)%>";
  defline "<%pi_2 x%>"  "<%z%>" ~side:"if <%exists y:T, x=(y,z)%>";
  defline "<%(i,j) x%>" "<%(i y,j z)%>" ~side:"if <%x=(y,z)%>, for <%i%> and <%j%> partial functions on <%T%>"; (* arnaud: il faut 'etendre ,ca aux relations en g'en'eral *)
  defline "<%epsilon x%>" "<%()%>";
  defline "<%() x%>" "<%()%>" ~side:"if <%x=()%>";
]}

The set of instruction <%Phi%> is chosen to be the smallest set containing the partial functions <%{delta;pi_1;pi_2;epsilon;();r1}%> and closed by <%(ph,ph)%>. This set of instructions has been chosen to correspond to the presentation of cartesian products and terminal elements in categories as adjunctions.

We call {emph"tree machine"} a machine of type <%(T,Phi)%>. Notice that, contrary to Turing machines, tree machines are not parametrised by an alphabet: the tree structure offers enough power on its own.

Tree machines, by virtue of the <%(ph,ph)%> instruction scheme, has an infinite number of instructions which make it possible to observe the tree and modify it at arbitrary depths. However each individual instruction affects trees at a bounded depth, which is considered a constant time operation in functional language, which is the important property we want to ensure. As we shall see in Section~{ref_ s_finitetype}, this choice of an infinite set of instruction is pure convenience: a finite set suffices.


{subsection"A language of guards and actions"}

One way to read the instruction in <%Phi%>, is to think of them as combinators giving means to match tree prefixes and rearrange the corresponding subtrees. That is, instructions of tree machines perform pattern-matching. We shall give an alternative set of instructions for tree machines which is suggestive of the pattern-matching notation functional programmers all know and love.

We write <%gamma => alpha%> for a partial function whose domain is denoted by the {emph"guard"}, or pattern, <%gamma%> and whose functional action is denoted by the {emph"action"} <%alpha%>. Both <%gamma%> and <%alpha%> are trees with variables, with the following restrictions: variables occur at most once <%gamma%>, and all variables of <%alpha%> appear in <%gamma%> (variables in <%gamma%> bind variables in <%alpha%>). We may use <%gamma_1 => alpha_1 | gamma_2 => alpha_2%> instead of <%(gamma_1 => alpha_1)+(gamma_2 => alpha_2)%> when <%gamma_1%> and <%gamma_2%> denote disjoint domains ({emph"i.e."} <%gamma_1%> and <%gamma_2%> are not unifiable). We can also use the wildcard pattern ``<%\_%>'' to represent a variable in <%gamma%> which does not bind a variable in <%alpha%>.

For instance, the instructions of <%Phi%> can be represented using this notations as follows:
{definition [
  defline "<%r1%>" "<%x => x%>";
  defline "<%delta%>" "<%x => (x,x)%>";
  defline "<%pi_1%>" "<%(x,\_) => x%>";
  defline "<%pi_2%>" "<%(\_,y) => y%>";
  defline "<%(i,j)%>" "<%(gamma_i,gamma_j) => (alpha_i,alpha_j)%>" ~side:"for <%i=gamma_i=>gamma_j%> and <%j=gamma_j=>alpha_j%>";
  defline "<%epsilon%>" "<%\_ => ()%>";
  defline "<%()%>" "<%() => ()%>";
]}

Conversely, the language of guard and action is subsumed by <%Phi%>, which we shall use as a definition rather than giving and independent definition and prove it as a theorem (which would be, of course, theoretically possible but not practically useful). The definition is lexicographically recursive on the subterm ordering of <%gamma%> then that of <%alpha%>.
{definition [
  defline "<%(x => x)%>" "<%r1%>";
  defline "<%(x => ())%>" "<%epsilon%>";
  defline "<%(() => ())%>" "<%()%>";
  defline "<%((gamma_1,gamma_2) => ())%>" "<%((gamma_1 => ()),(gamma_2=>())).epsilon%>";
  defline "<%((gamma_1,gamma_2) => x)%>" "<%((gamma_1 => x),(gamma_2=>())).pi_1%>" ~side:"when <%x%> occurs in <%gamma_1%>";
  defline "<%((gamma_1,gamma_2) => x)%>" "<%((gamma_1 => ()),(gamma_2=>x)).pi_2%>" ~side:"when <%x%> occurs in <%gamma_2%>";
  defline "<%(gamma => (alpha_1,alpha_2))%>" "<%delta.((gamma=>alpha_1),(gamma=>alpha_2))%>";
]}
The language of pattern and action can be used to conveniently defined the following examples:
{definition [
  defline "<%sigma%>" "<%((x,y) => (y,x))%>";
  defline "" "<%delta.((epsilon,r1).pi_2,(r1,epsilon).pi_1)%>";
  defline "<%push%>" "<%((x,y),z) => (x,(y,z))%>";
  defline "" "<%delta.(((r1,epsilon).pi_1,epsilon).pi_1,delta.(((epsilon,r1).pi_2,epsilon).pi_1,((epsilon,epsilon).epsilon,r1).pi_2))%>";
 ]}
This procedure does not give the smallest possible definition in terms of <%Phi%> of <%sigma%> and <%push%> (or of pretty much anything for that matter). Here are better candidates for this particular award:
{definition [
  defline "<%sigma%>" "<%delta.(pi_2,pi_1)%>";
  defline "<%push%>" "<%delta.(pi_1.pi_1 , delta.(pi_2.pi_1,pi_2))%>";
 ]}
In either case, however, it is fair to claim that the language of guard and patterns gives a clearer account of the intent and semantics of instructions than the more elementary <%Phi%>. In consequence we will peruse the guard and actions in the remainder of the article.

{subsection"Constants"}

It will be useful to embed natural numbers in trees. Any embedding will do. We choose a binary encoding for the sake of fun and compactness:
{definition [
  defline "<%0%>" "<%()%>";
  defline "<%2*n+1%>" "<%((),n)%>";
  defline "<%2*n+2%>" "<%(((),()),n)%>"
 ]}

A direct consequence of this encoding is that any finite set of symbols can be easily represented as particular trees, by mapping them to arbitrary distinct natural numbers. We will do so quite liberally.

{subsection"Zipper" ~label:s_zipper}

In~{cite"Huet1997"} Huet presents a purely functional data structure, the {emph"zipper"} to implement ``pointers'' in trees, {emph"i.e."} a way to walk through a tree from parent to child or back in constant time and to replace the pointed subtree also in constant time.

The zipper can be adapted to the tree machines as a set of instructions. The zipper instruction provide us with an ability we have not had so far: modifying a subtree at an {emph"a priori"} unbounded depth in a tree.

The idea is that a zipper is represented as a pair <%(c,t)%> of the pointed subtree <%t%> together with a ``reversed tree'' <%c%> which represents the context and gives enough information to rebuild the tree when walking up towards the parent (with the instruction <%up%> below). Here are the relevant instructions:
{definition [
  defline "<%open%>" "<%x => ((),x)%>";
  defline "<%left%>" "<%(x,(y,z)) => ((0,(x,z)),y)%>";
  defline "<%right%>" "<%(x,(y,z)) => ((1,(x,y)),z)%>";
  defline "<%up%>" "<%((0,(x,z)),y) => (x,(y,z)) | ((1,(x,y)),z) => (x,(y,z))%>";
  defline "<%exit%>" "<%((),x) => x%>";
]}
Where <%open%> and <%exit%> transform a tree into a pointer to its root and back, <%left%> walks down to the left child (and marks, in the reversed tree, with <%0%> that it did walk down left), <%right%> to the right child, and <%up%> walks up to the parent, and reconstruct the tree according to the mark left by either <%left%> or <%right%>. For further detail, the reader unfamiliar with these concepts is deeply encouraged to read Huet's paper~{cite"Huet1997"}

(* The instruction <%zip = up^*.exit%> turns a focused tree back into a tree. Because it uses an iteration, it is not an instruction which has constant time. However, when it is known that the depth of the focused subtree is at most a constant <%n%> we can equivalently use the constant time instruction <%(r1+up^1+%{ldots}%+up^n).exit%>, we may still use <%zip%> as a shorthand. *)
"

let unit_symb = "<%()%>"

let translations = "{section"Comparisons"}

With the basic material now set in place, we can now turn to the use of the tree machine as a computational complexity model. It is important to be precise on what is meant here: clearly, complexity classes are very robust, and it does not matter what computation model is taken to define them; the tree machine is no exception. However, for more fine grained accounts of asymptotic complexity, the model will matter a lot: in Turing machines already, multiple-tape Turing machines can provide a quadratic speedup over single-tape ones.

It is this sort of complexity that is our concern, and the claim of this article is that the tree machine is a good complexity model for purely functional computations.

{subsection"Turing machines as tree machines"}

It is straightforward to implement Turing machines as tree-machines: fixing a coding for the alphabet, we arrange the tree to be a pair <%(L,R)%> of lists of symbols. The list <%R=(%{mathsf$a$}%_0, (%{mathsf$a$}%_1, (%{ldots}%,%{unit_symb^^ldots}%)))%> represents the part of the tape just under and to the right of the head (<%%{mathsf$a$}%_0%> is the symbol under the head). The reversed list <%L=(((%{ldots^^unit_symb}%,%{ldots}%),%{mathsf$b$}%_2),%{mathsf$b$}%_1)%> represents the part of the tape which sits to the left of the head. With this representation there is no need for a special symbol to stand at empty slots on the tape: instead the symbol under the head is empty when <%R%> is the empty list <%()%>.

The instruction of Turing machines are implemented as instruction of the tree machines (in guard-and-action style):
{itemize [
  "Write symbol {mathsf$a$} under the head: <%(L,(\_,R)) => (L,(%{mathsf$a$}%,R)) | (x,()) => (x,(%{mathsf$a$}%,()))%> (the second case extends the tape if we reached the end)";
  "Move right: <%(L,(x,R)) => ((L,x),R)%>";
  "Move left: <%((L,x),R) => (L,(x,R))%>";
  "Check that symbol {mathsf$a$} is under the head: <%(L,(%{mathsf$a$}%,R)) => (L,(%{mathsf$a$}%,R))%>";
]}
Therefore, a Turing machines is translated to a tree machine with the same state and transitions, except the instructions labelling transitions are replaced with their respective implementation as tree machine instructions.

This translation highlights a point which is occasionally overlooked: Turing machines are not a very good model of imperative programs. Turing machines can be simulated in constant time in a purely functional language, and so can multiple-tape Turing machines. To get a better model of imperative programs, we shall turn to random-access machines in Section~{ref_ s_ram}.

{subsection"A finite type for tree machines"~label:s_finitetype}

In order to build the converse translation of tree machines into Turing machines, it will be convenient for the set instruction to be presented by a finite set <%Psi%>. To obtain this finite presentation we will use the zipper instructions from Section~{ref_ s_zipper}. To be more specific, we will implement the instruction <%(i,j) IN Phi%> as a (finite) sequence of zipper operations which will walk through the tree to apply the appropriate actions to the appropriate subtrees.

Remember that a zipper is a pair <%(c,t)%> of a focused subtree <%t%> together with a reversed tree <%c%> representing the necessary context to rebuild the tree. Our goal is to lift the instructions of <%Phi%> so that they apply to the focused subtree instead of the root of the complete tree. In other words, we are looking for a <%[[i]]=(1,i)%> for each <%i IN Phi%>.

This property that <%[[i]]=(1,i)%> acts as a perfectly fine definition for each of the generators of <%Phi%>:
{Prelude.definition [
  defline "<%[[r1]]%>" "<%(r1,r1)%>";
  defline "<%[[()]]%>" "<%(r1,())%>";
  defline "<%[[epsilon]]%>" "<%(r1,epsilon)%>";
  defline "<%[[pi_1]]%>" "<%(r1,pi_1)%>";
  defline "<%[[pi_2]]%>" "<%(r1,pi_2)%>";
  defline "<%[[delta]]%>" "<%(r1,delta)%>";
 ]}
For the case <%(i,j)%> however, in order to avoid introducing infinitely many instructions in <%Psi%>, we need to find an alternative definition in terms of the zipper operations. We define <%[[(i,j)]]%> recursively as follows:
{Prelude.definition [
  defline "<%[[(i,j)]]%>" "<%left.[[i]].up.right.[[j]].up%>";
 ]}
It is a straightforward exercise of symbol pushing to verify that indeed, by induction, <%[[(i,j)]] = (r1,(i,j))%>.

Every instruction <%i IN Phi%> can be implemented as <%open.[[i]].exit%>. Therefore we can take the set <%Phi%> as being
{displaymath
  "<%Psi = {open;left;right;up;exit;(r1,r1);(r1,());(r1,epsilon);(r1,pi_1);(r1,pi_2);(r1,delta)}%>"}


{subsection"Tree machines as Turing machines"}

Translating tree machines into Turing machines is not as direct as the converse. One way to translate trees into word so that it fits a Turing machine tape is to use the Polish notations: we take the alphabet to include <%{%{mathsf$p$}%;%{mathsf$u$}%}%> (for {emph"pair"} and {emph"unit"} respectively). The tree <%((),((),()))%> is then translated to <%pupuu%>.

As always, giving a concrete definition of a Turing machine~--~or even of a translation to Turing machine~--~is not very easy nor particularly enlightening, and, in fact, would lead us way over the page limit. On the other hand, equipped with the finite presentation of Section~{ref_ s_finitetype}, it is quite clear that it can in principle be done.

Let us sketch how such a construction could be achieved. In a first step we can ignore the <%open%> and <%exit%> instruction, and assume we are always working on a zipper. The rational is that <%exit.open%> acts as the identity on a zipper which is focused at the root, so we may simply represent non-zipper trees as zipper focused at the root and both instructions become the identity. One may be tempted to replace calls to <%exit%> to test that the context is a {mathsf$u$}, but this is not even necessary as it is an invariant of the translation.

To represent the zipper <%(c,t)%>, the most convenient way is to have two tapes, one holding <%c%> and the other holding <%t%>. A third tape will be useful as a stack to discover subtree boundaries when implementing <%pi_1%> and <%pi_2%>

To manage the Polish notation, we can take a two-tape Turing machine, the first tape holds the current tree, and the second tape can hold a stack to find the boundaries of the two subtrees to implement <%pi_1%> and <%pi_2%> or a buffer with the tree to copy to implement <%delta%>.

The instructions of tree machines are not translated as constant time machines. However, they are all in a polynomial <%P%> of the current size of the tape. Hence, if the complexity of a tree machine is <%O \(f n\)%>, then the corresponding Turing machine has complexity <%O \(P \(f n\) * f n\)%>, which is in the same complexity class.

Therefore, tree machines and Turing machines have the same complexity classes (we only discuss non-deterministic machines in this article, but there is a natural notion of deterministic tree machine, and the translation can be arranged to preserve determinism). However, the translation from tree machines to Turing machines is non-trivial both in term of slowdown of the translated machine and complexity of the translation itself. It would be quite hard to get an explicit description of the translation. On the other hand the translation of Turing machines into tree machines is quite direct. Tree machines are a significant improvement over Turing machines.

{subsection"Tree machines as random-access machines" ~label:s_ram}

A more natural encoding of Tree machines is in random access machines -- another example of Eilenberg machine. In this translation, a tree is an address at this address there is <%0%> if the tree is empty, and <%1%> if the tree is a pair. In the latter the two following addresses contain the addresses of the two subtrees.

In this translation, it is easy to implement the instructions of the tree machine as they are just pointer dereferencing and copy. The difficulties are:
{itemize[
  "Memory allocation: finding empty space to store a tree";
  "Garbage collection: to keep the same memory profile";
]}
Both are linear in a random access machine.

Memory allocation and garbage collection are usually assumed away in modern programming language, making the Tree machine a good complexity model of modern programming languages. On the other hand, it does not have constant-time-access arrays, which the random access machines provides and are available in programming language -- except purely functional ones.

{section"Encoding {lambda}-calculus" ~label:s_lambda}

Using explicit substitution {lambda}{upsilon}-calculus~{cite"Lescanne1994"}:
{Prelude.definition[
  defline $({lambda}u)v$ $u[v]$;
  defline $({lambda}u)[s]$ ${lambda}(u[{uparrow_}s])$;
  defline $(u v)[s]$ $(u[s])(v[s])$;
  defline $0[v]$ $v$;
  defline $(n+1)[v]$ $n+1$;
  defline $0[{uparrow_}s]$ $0$;
  defline $(n+1)[{uparrow_}s]$ $n[s][{uparrow}]$;
  defline $n[{uparrow}s]$ $n+1$;
]}

We will use the non-determinism of tree machines to represent the non-determinism of {beta}-reduction (the converse is not possible).

We fix an encoding for the following list of symbols <%{lam;app;succ;nought;subst;term;lift;shift}%>. First we need zipper-like constructions to navigate through terms:
{Prelude.definition [
  defline "<%open%>" "<%u => ((),u)%>";
  defline "<%down_lam%>" "<%(c,(lam,u)) => ((c,lam),u)%>";
  defline "<%down_sigma%>" "<%(c,(subst,(u,s))) => ((c,(subst,s)),u)%>";
  defline "<%left%>" "<%(c,(app,(u,v))) => ((c,(app,(0,v))),u)%>";
  defline "<%right%>" "<%(c,(app,(u,v))) => ((c,(app,(1,u))),v)%>";
  defline "<%up_lam%>" "<%((c,lam),u) => (c,(lam,u))%>";
  defline "<%up_sigma%>" "<%((c,(subst,s)),u) => (c,(subst,(u,s)))%>";
  defline "<%up_app%>" "<%((c,(app,(0,v))),u) => (c,(app,(u,v))) | ((c,(app,(1,u))),v) => (c,(app,(u,v)))%>";
  defline "<%up%>" "<%up_lam+up_sigma+up_app%>";
  defline "<%exit%>" "<%((),u) => u%>";
  defline "<%move%>" "<%down_lam+down_sigma+left+right+up%>";
  defline "<%zip%>" "<%up^*.exit%>";
 ]}
Now we implement the rules in a context independent manner:
{Prelude.definition [
  defline "<%beta%>" "<%(app,((lam,u),v)) => (subst,(u,(term,v)))%>";
  defline "<%lam_sigma%>" "<%(subst,((lam,u),s)) => (lam,(subst,(u,(lift,s))))%>";
  defline "<%app_sigma%>" "<%(subst,((app,(u,v)),s)) => (app,((subst,(u,s)),(subst,(v,s))))%>";
  defline "<%nought_term%>" "<%(subst,(nought,(term,v))) => v%>";
  defline "<%succ_term%>" "<%(subst,((succ,n),(term,\_))) => succ n%>";
  defline "<%nought_lift%>" "<%(subst,(nought,(lift,\_))) => nought%>";
  defline "<%succ_lift%>" "<%(subst,((succ,n),(lift,s))) => (subst,((subst,(x,s)),shift))%>";
  defline "<%var_shift%>" "<%(subst,(nought,shift)) => (succ,nought) | (subst,((succ,n),shift)) => (succ,(succ,n))%>";
  defline "<%rule_sigma%>" "<%lam_sigma+app_sigma+nought_term+succ_term+nought_lift+succ_lift+var_shift%>"; 
  defline "<%rule%>" "<%beta+rule_sigma%>";
]}
A step of {lambda}{upsilon}-calculus is implemented as:
{Prelude.definition[
  defline "<%step%>" "<%open.move^*.(1,rule).zip%>";
]}
Reduction is then <%step^*%>.

To better represent actual {lambda}-calculus, a step of reduction should correspond to exactly one {beta}-reduction and substitutions should be eliminated. To do so we extend or alphabet of symbol with a symbol <%ok%> together with the following rules:
{Prelude.definition[
  defline "<%nought_ok%>" "<%nought => (ok,nought)%>";
  defline "<%succ_ok%>" "<%(succ,n) => (ok,(succ,n))%>";
  defline "<%lam_ok%>" "<%(lam,(ok,u)) => (ok,(lam,u))%>";
  defline "<%app_ok%>" "<%(app,((ok,u),(ok,v))) => (ok,(app,(u,v)))%>";
  defline "<%rule_ok%>" "<%nought_ok+succ_ok+lam_ok+app_ok%>";
  defline "<%check_ok%>" "<%(ok,u) => u%>";
]}
Then, we can encode a step of {lambda}-calculus with:
{Prelude.definition[
  defline "<%step%>" "<%open.move^*.(1,beta.rule_sigma^*.rule_ok^*.check_ok).zip%>";
]}
"

(*** Emit document ***)

open Llncs

let mines = new_institution "MINES ParisTech"

let funding =
  "This research has received funding from the European Research Council under the FP7 grant agreement 278673, Project MemCAD"

let title = {
  title = "The tree machine{command\"thanks\" [A,funding] A}";
  running_title = None
}

let authors = [
  { name = "Arnaud Spiwack";
    email = "arnaud@spiwack.net";
    institution = mines;
    running_name = None;
  };
]

let d = concat [
  intro;
  eilenberg;
  definition;
  translations;
  command \"bibliography\" [A,"library"] A;
]

let packages = [
  "inputenc" , "utf8" ;
  "fontenc" , "T1" ;
  "textcomp", "";
  "microtype" , "" ;
]

let prelude = concat_with_sep [
  usepackage "hyperref";
  text\"\\\\let\\\\oldampersand\\\\&\";
  text\"\\\\renewcommand*\\\\&{{\\\\itshape\\\\oldampersand}}\";
  command \"bibliographystyle\" [T,"plain"] T;
] par

let file = \"treemachine.tex\"


let _ = emit ~file (document
                             ~title
                             ~authors
                             ~abstract
                             ~prelude
                             ~packages
                             d)
